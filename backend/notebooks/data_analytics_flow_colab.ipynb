{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1f57b21",
   "metadata": {},
   "source": [
    "## Data Analytics: Flujo End-to-End (Google Colab) 🌟\n",
    "\n",
    "Este notebook reproduce el flujo del notebook original y ha sido adaptado para ejecutarse directamente en **Google Colab**, asegurando compatibilidad con los módulos del proyecto y facilitando la ejecución completa del pipeline de datos.\n",
    "\n",
    "## Descripción del flujo\n",
    "\n",
    "El notebook está diseñado para cubrir el ciclo completo de **Data Analytics**, desde la generación del dataset hasta la visualización interactiva de resultados, pasando por validaciones y cálculos de métricas.\n",
    "\n",
    "## Pasos del flujo\n",
    "\n",
    "1. **Clonar el repositorio**  \n",
    "   Habilita la importación de los módulos `app.*` necesarios para el correcto funcionamiento del proyecto.\n",
    "\n",
    "2. **Instalar dependencias**  \n",
    "   Se utilizan comandos `!pip` para asegurar que todas las librerías requeridas estén disponibles en el entorno de Colab.\n",
    "\n",
    "3. **Configurar rutas y entorno**  \n",
    "   Ajuste de `sys.path` y directorios en `/content` para garantizar la carga correcta de scripts y datos.\n",
    "\n",
    "4. **Generar el dataset** (idempotente)  \n",
    "   Creación de un dataset de productos simulado, listo para análisis, y carga en un **DataFrame** de pandas.\n",
    "\n",
    "5. **Verificar integridad de los datos**  \n",
    "   Comprobación de columnas obligatorias, valores nulos, rangos numéricos, y consistencia general del dataset.\n",
    "\n",
    "6. **Calcular métricas de productos**  \n",
    "   Procesamiento de métricas clave y exportación de los resultados a formato **JSON**.\n",
    "\n",
    "7. **Análisis exploratorio de datos**  \n",
    "   Generación de estadísticas básicas y resúmenes por categoría, guardando resultados en **JSON** para documentación y seguimiento.\n",
    "\n",
    "8. **Visualizaciones interactivas**  \n",
    "   Uso de **Plotly** para explorar tendencias, distribuciones y relaciones entre variables de manera intuitiva.\n",
    "\n",
    "> **Nota:** Se recomienda ejecutar las celdas en orden siguiendo: **Runtime > Run all** para asegurar que todo el flujo se ejecute correctamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c27e8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Contexto de ejecución y utilidades\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import importlib\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules or os.environ.get(\"COLAB_RELEASE_TAG\") is not None\n",
    "BASE_DIR = Path(\"/content\") if IN_COLAB else Path.cwd()\n",
    "REPO_URL = \"https://github.com/vhcontre/data-analytics-con-python.git\"\n",
    "REPO_DIR = BASE_DIR / \"data-analytics-con-python\"\n",
    "BACKEND_ROOT = REPO_DIR / \"backend\"\n",
    "\n",
    "print(f\"Python: {sys.version.split()[0]}\")\n",
    "print(f\"IN_COLAB: {IN_COLAB}\")\n",
    "print(f\"REPO_DIR: {REPO_DIR}\")\n",
    "print(f\"BACKEND_ROOT: {BACKEND_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd2d724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Clonar el repositorio (si no existe) para habilitar imports 'app.*'\n",
    "if not REPO_DIR.exists():\n",
    "    print(\"Clonando repositorio...\")\n",
    "    subprocess.run([\"git\", \"clone\", \"--depth\", \"1\", REPO_URL, str(REPO_DIR)], check=True)\n",
    "else:\n",
    "    print(\"Repositorio ya presente; saltando clonación.\")\n",
    "\n",
    "# Comprobación rápida de carpetas clave\n",
    "for p in [BACKEND_ROOT / \"app\", BACKEND_ROOT / \"datasets\", BACKEND_ROOT / \"reports\"]:\n",
    "    print(p, 'exists' if p.exists() else 'missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81580908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Instalar dependencias necesarias\n",
    "# Nota: Usamos !pip para que los paquetes queden en el kernel actual.\n",
    "!pip install -q -U pip\n",
    "!pip install -q -r /content/data-analytics-con-python/backend/requirements.txt\n",
    "\n",
    "# Chequeo rápido de librerías clave\n",
    "for mod in (\"numpy\", \"pandas\", \"plotly\"):\n",
    "    try:\n",
    "        m = importlib.import_module(mod)\n",
    "        print(f\"{mod}=={getattr(m, '__version__', 'unknown')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Falta o falla {mod}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b20a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Configurar rutas y sys.path para importar 'app.*'\n",
    "if str(BACKEND_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(BACKEND_ROOT))\n",
    "\n",
    "DATASETS_DIR = BACKEND_ROOT / \"datasets\"\n",
    "REPORTS_DIR = BACKEND_ROOT / \"reports\"\n",
    "DATASETS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CSV_PATH = DATASETS_DIR / \"product_dataset.csv\"\n",
    "\n",
    "print(f\"DATASETS_DIR: {DATASETS_DIR}\")\n",
    "print(f\"REPORTS_DIR: {REPORTS_DIR}\")\n",
    "print(f\"CSV_PATH: {CSV_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c82c823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Generar dataset (idempotente) y cargarlo\n",
    "import pandas as pd\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Revalidar variables de ruta por si se ejecuta esta celda aislada\n",
    "try:\n",
    "    CSV_PATH\n",
    "except NameError:\n",
    "    from pathlib import Path\n",
    "    REPO_DIR = Path(\"/content\") / \"data-analytics-con-python\"\n",
    "    BACKEND_ROOT = REPO_DIR / \"backend\"\n",
    "    if str(BACKEND_ROOT) not in sys.path:\n",
    "        sys.path.insert(0, str(BACKEND_ROOT))\n",
    "    DATASETS_DIR = BACKEND_ROOT / \"datasets\"\n",
    "    DATASETS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    CSV_PATH = DATASETS_DIR / \"product_dataset.csv\"\n",
    "\n",
    "# Generar sólo si no existe (usar CLI del módulo; main() no acepta args posicionales)\n",
    "if not CSV_PATH.exists():\n",
    "    subprocess.run([\n",
    "        sys.executable, \"-m\", \"app.scripts.create_product_dataset\",\n",
    "        \"--num-samples\", \"200\", \"--seed\", \"42\", \"--out\", str(CSV_PATH)\n",
    "    ], cwd=str(BACKEND_ROOT), check=True)\n",
    "else:\n",
    "    print(\"Dataset ya existe; saltando generación.\")\n",
    "\n",
    "# Cargar\n",
    "assert CSV_PATH.exists(), f\"No se encontró el CSV en {CSV_PATH}\"\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"Dataset cargado:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c59b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Verificar integridad (modo no estricto para continuar el flujo)\n",
    "from app.scripts.check_dataset_integrity import main as chk_main\n",
    "\n",
    "# Revalidar CSV_PATH si se ejecuta esta celda aislada\n",
    "try:\n",
    "    CSV_PATH\n",
    "except NameError:\n",
    "    from pathlib import Path\n",
    "    import sys\n",
    "    REPO_DIR = Path(\"/content\") / \"data-analytics-con-python\"\n",
    "    BACKEND_ROOT = REPO_DIR / \"backend\"\n",
    "    if str(BACKEND_ROOT) not in sys.path:\n",
    "        sys.path.insert(0, str(BACKEND_ROOT))\n",
    "    DATASETS_DIR = BACKEND_ROOT / \"datasets\"\n",
    "    DATASETS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    CSV_PATH = DATASETS_DIR / \"product_dataset.csv\"\n",
    "\n",
    "exit_code = chk_main([\"--path\", str(CSV_PATH)])\n",
    "print(\"Integrity check exit code:\", exit_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56da2ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Calcular métricas y guardar JSON\n",
    "from app.scripts.calculate_product_metrics import main as met_main\n",
    "from pathlib import Path\n",
    "\n",
    "# Revalidar rutas si se ejecuta aislada\n",
    "try:\n",
    "    REPORTS_DIR\n",
    "    CSV_PATH\n",
    "except NameError:\n",
    "    from pathlib import Path as _Path\n",
    "    import sys\n",
    "    REPO_DIR = _Path(\"/content\") / \"data-analytics-con-python\"\n",
    "    BACKEND_ROOT = REPO_DIR / \"backend\"\n",
    "    if str(BACKEND_ROOT) not in sys.path:\n",
    "        sys.path.insert(0, str(BACKEND_ROOT))\n",
    "    REPORTS_DIR = BACKEND_ROOT / \"reports\"\n",
    "    REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    DATASETS_DIR = BACKEND_ROOT / \"datasets\"\n",
    "    DATASETS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    CSV_PATH = DATASETS_DIR / \"product_dataset.csv\"\n",
    "\n",
    "# Garantizar un Path tipado para evitar Union[Unbound|Path]\n",
    "if 'REPORTS_DIR' in globals() and isinstance(REPORTS_DIR, Path):\n",
    "    _REPORTS_DIR: Path = REPORTS_DIR\n",
    "else:\n",
    "    _BACKEND = (Path(\"/content\") / \"data-analytics-con-python\" / \"backend\")\n",
    "    _REPORTS_DIR = _BACKEND / \"reports\"\n",
    "    _REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "metrics_path = _REPORTS_DIR / \"metrics.json\"\n",
    "_ = met_main([\"--path\", str(CSV_PATH), \"--json-out\", str(metrics_path)])\n",
    "\n",
    "metrics = json.loads(metrics_path.read_text(encoding=\"utf-8\"))\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a91590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Análisis exploratorio y guardar JSON\n",
    "from app.scripts.exploratory_analysis import main as exp_main\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Revalidar rutas si se ejecuta aislada\n",
    "try:\n",
    "    REPORTS_DIR\n",
    "    CSV_PATH\n",
    "except NameError:\n",
    "    from pathlib import Path as _Path\n",
    "    import sys\n",
    "    REPO_DIR = _Path(\"/content\") / \"data-analytics-con-python\"\n",
    "    BACKEND_ROOT = REPO_DIR / \"backend\"\n",
    "    if str(BACKEND_ROOT) not in sys.path:\n",
    "        sys.path.insert(0, str(BACKEND_ROOT))\n",
    "    REPORTS_DIR = BACKEND_ROOT / \"reports\"\n",
    "    REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    DATASETS_DIR = BACKEND_ROOT / \"datasets\"\n",
    "    DATASETS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    CSV_PATH = DATASETS_DIR / \"product_dataset.csv\"\n",
    "\n",
    "# Garantizar Path tipado para reports\n",
    "if 'REPORTS_DIR' in globals() and isinstance(REPORTS_DIR, Path):\n",
    "    _REPORTS_DIR: Path = REPORTS_DIR\n",
    "else:\n",
    "    _BACKEND = (Path(\"/content\") / \"data-analytics-con-python\" / \"backend\")\n",
    "    _REPORTS_DIR = _BACKEND / \"reports\"\n",
    "    _REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "expl_summary_path = _REPORTS_DIR / \"exploratory_summary.json\"\n",
    "_ = exp_main([\"--path\", str(CSV_PATH), \"--json-out\", str(expl_summary_path)])\n",
    "\n",
    "summary = json.loads(expl_summary_path.read_text(encoding=\"utf-8\"))\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5268be70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Visualizaciones con Plotly\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Revalidar df y metrics si se ejecuta aislada\n",
    "try:\n",
    "    df\n",
    "except NameError:\n",
    "    from pathlib import Path\n",
    "    import sys\n",
    "    REPO_DIR = Path(\"/content\") / \"data-analytics-con-python\"\n",
    "    BACKEND_ROOT = REPO_DIR / \"backend\"\n",
    "    if str(BACKEND_ROOT) not in sys.path:\n",
    "        sys.path.insert(0, str(BACKEND_ROOT))\n",
    "    DATASETS_DIR = BACKEND_ROOT / \"datasets\"\n",
    "    CSV_PATH = DATASETS_DIR / \"product_dataset.csv\"\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "try:\n",
    "    metrics\n",
    "except NameError:\n",
    "    import json\n",
    "    REPORTS_DIR = BACKEND_ROOT / \"reports\"\n",
    "    metrics_path = REPORTS_DIR / \"metrics.json\"\n",
    "    if metrics_path.exists():\n",
    "        metrics = json.loads(metrics_path.read_text(encoding=\"utf-8\"))\n",
    "    else:\n",
    "        metrics = {}\n",
    "\n",
    "# Conteo por categoría\n",
    "if \"Category\" in df.columns:\n",
    "    counts = df[\"Category\"].value_counts(dropna=False).reset_index()\n",
    "    counts.columns = [\"Category\", \"count\"]\n",
    "    fig1 = px.bar(counts, x=\"Category\", y=\"count\", title=\"Conteo de productos por categoría\")\n",
    "    fig1.show()\n",
    "\n",
    "# Promedios globales\n",
    "avg_df = pd.DataFrame([\n",
    "    {\"metric\": \"BaseYield\", \"value\": metrics.get(\"average_base_yield\")},\n",
    "    {\"metric\": \"Cost\", \"value\": metrics.get(\"total_cost\")},\n",
    "    {\"metric\": \"EnvironmentalImpact\", \"value\": metrics.get(\"average_environmental_impact\")},\n",
    "]).dropna()\n",
    "if not avg_df.empty:\n",
    "    fig2 = px.bar(avg_df, x=\"metric\", y=\"value\", title=\"Métricas globales\")\n",
    "    fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1558f6d",
   "metadata": {},
   "source": [
    "### Notas importantes\n",
    "- En caso de errores de importación, es recomendable revisar y ejecutar nuevamente las primeras tres celdas.\n",
    "- Los archivos generados se almacenan en las siguientes rutas:\n",
    "  - `/content/data-analytics-con-python/backend/datasets`\n",
    "  - `/content/data-analytics-con-python/backend/reports`\n",
    "- Los archivos JSON resultantes pueden descargarse desde el panel de archivos de Colab.\n",
    "- ℹ️ Para repositorios privados, es necesario reemplazar la celda de clonación por un procedimiento de autenticación mediante token de GitHub.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
